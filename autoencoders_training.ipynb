{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import scanpy as sc\n",
    "\n",
    "from MaskedLinear import MaskedLinear, DetMaskLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read('mouse_retina_sbs.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_genes = adata.varm['I'].sum(1)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata._inplace_subset_var(select_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X-=adata.X.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 20\n",
    "BATCH_SIZE = 62\n",
    "LR = 0.005\n",
    "ALPHA1 = 0.24\n",
    "ALPHA2 = 0.17\n",
    "ALPHA3 = 0.18\n",
    "ALPHA4 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAutoencoder(nn.Module):\n",
    "    def __init__(self, n_vars, n_terms, n_latent, estimator='ST', f_eval='Mode'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            MaskedLinear(n_vars, n_terms, estimator, f_eval, bias=False),\n",
    "            nn.ELU(),\n",
    "            MaskedLinear(n_terms, n_latent, estimator, f_eval)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(n_latent, n_terms),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(n_terms, n_vars),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_func(I, alpha1, alpha2, alpha3, alpha4):\n",
    "    l2_loss = nn.MSELoss()\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def regularized_loss(X, Y, logits1, logits2):\n",
    "        sigm1 = torch.sigmoid(logits1)\n",
    "        sigm2 = torch.sigmoid(logits2)\n",
    "        return l2_loss(X, Y), alpha1*bce_loss(logits1, I.t())+alpha2*torch.mean(sigm1*(1-sigm1)), alpha3*torch.mean(sigm2)+alpha4*torch.mean(sigm2*(1-sigm2))\n",
    "    \n",
    "    return regularized_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(autoencoder, loss_func):\n",
    "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=LR)\n",
    "    \n",
    "    t_X = torch.from_numpy(adata.X)\n",
    "    \n",
    "    logits1 = autoencoder.encoder[0].logits\n",
    "    logits2 = autoencoder.encoder[2].logits\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        autoencoder.train()\n",
    "        for step in range(int(adata.n_obs/BATCH_SIZE)):\n",
    "            batch = torch.from_numpy(adata.chunk_X(BATCH_SIZE))\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = 0\n",
    "            \n",
    "            for sample in batch:\n",
    "                sample = sample[None, :]\n",
    "                encoded, decoded = autoencoder(sample)\n",
    "                \n",
    "                loss = sum(loss_func(decoded, sample, logits1, logits2))/BATCH_SIZE\n",
    "                loss.backward()\n",
    "                \n",
    "                batch_loss += loss.data\n",
    "            \n",
    "            optimizer.step()\n",
    "            if step % 100 == 0: print('Epoch: ', epoch, '| batch train loss: %.4f' % batch_loss.numpy())\n",
    "        autoencoder.eval()\n",
    "        _, t_decoded = autoencoder(t_X)\n",
    "        \n",
    "        t_loss = loss_func(t_decoded, t_X, logits1, logits2)\n",
    "        t_loss = [sum(t_loss)] + list(t_loss)\n",
    "        t_loss = [l.data.numpy() for l in t_loss]\n",
    "        \n",
    "        print('Epoch: ', epoch, '-- total train loss: %.4f=%.4f+%.4f+%.4f' % tuple(t_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = MaskedAutoencoder(adata.n_vars, len(adata.uns['terms']), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = torch.from_numpy(adata.varm['I']).float()\n",
    "loss_func = get_loss_func(I, ALPHA1, ALPHA2, ALPHA3, ALPHA4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | batch train loss: 0.5576\n",
      "Epoch:  0 | batch train loss: 0.4497\n",
      "Epoch:  0 | batch train loss: 0.4200\n",
      "Epoch:  0 -- total train loss: 0.4096=0.1723+0.1348+0.1025\n",
      "Epoch:  1 | batch train loss: 0.4080\n",
      "Epoch:  1 | batch train loss: 0.3522\n",
      "Epoch:  1 | batch train loss: 0.3300\n",
      "Epoch:  1 -- total train loss: 0.3221=0.1534+0.0948+0.0739\n",
      "Epoch:  2 | batch train loss: 0.3282\n",
      "Epoch:  2 | batch train loss: 0.2913\n",
      "Epoch:  2 | batch train loss: 0.2778\n",
      "Epoch:  2 -- total train loss: 0.2733=0.1489+0.0711+0.0533\n",
      "Epoch:  3 | batch train loss: 0.2831\n",
      "Epoch:  3 | batch train loss: 0.2632\n",
      "Epoch:  3 | batch train loss: 0.2395\n",
      "Epoch:  3 -- total train loss: 0.2349=0.1397+0.0561+0.0391\n",
      "Epoch:  4 | batch train loss: 0.2400\n",
      "Epoch:  4 | batch train loss: 0.2299\n",
      "Epoch:  4 | batch train loss: 0.2143\n",
      "Epoch:  4 -- total train loss: 0.2096=0.1341+0.0459+0.0296\n",
      "Epoch:  5 | batch train loss: 0.2149\n",
      "Epoch:  5 | batch train loss: 0.2044\n",
      "Epoch:  5 | batch train loss: 0.1931\n",
      "Epoch:  5 -- total train loss: 0.1902=0.1284+0.0386+0.0232\n",
      "Epoch:  6 | batch train loss: 0.2165\n",
      "Epoch:  6 | batch train loss: 0.1788\n",
      "Epoch:  6 | batch train loss: 0.1746\n",
      "Epoch:  6 -- total train loss: 0.1785=0.1269+0.0329+0.0187\n",
      "Epoch:  7 | batch train loss: 0.1821\n",
      "Epoch:  7 | batch train loss: 0.1823\n",
      "Epoch:  7 | batch train loss: 0.1770\n",
      "Epoch:  7 -- total train loss: 0.1701=0.1261+0.0284+0.0156\n",
      "Epoch:  8 | batch train loss: 0.1733\n",
      "Epoch:  8 | batch train loss: 0.1715\n",
      "Epoch:  8 | batch train loss: 0.1670\n",
      "Epoch:  8 -- total train loss: 0.1637=0.1256+0.0248+0.0133\n",
      "Epoch:  9 | batch train loss: 0.1615\n",
      "Epoch:  9 | batch train loss: 0.1679\n",
      "Epoch:  9 | batch train loss: 0.1545\n",
      "Epoch:  9 -- total train loss: 0.1585=0.1250+0.0219+0.0116\n",
      "Epoch:  10 | batch train loss: 0.1588\n",
      "Epoch:  10 | batch train loss: 0.1606\n",
      "Epoch:  10 | batch train loss: 0.1525\n",
      "Epoch:  10 -- total train loss: 0.1544=0.1247+0.0195+0.0102\n",
      "Epoch:  11 | batch train loss: 0.1472\n",
      "Epoch:  11 | batch train loss: 0.1468\n",
      "Epoch:  11 | batch train loss: 0.1550\n",
      "Epoch:  11 -- total train loss: 0.1509=0.1242+0.0175+0.0092\n",
      "Epoch:  12 | batch train loss: 0.1525\n",
      "Epoch:  12 | batch train loss: 0.1533\n",
      "Epoch:  12 | batch train loss: 0.1592\n",
      "Epoch:  12 -- total train loss: 0.1481=0.1239+0.0159+0.0084\n",
      "Epoch:  13 | batch train loss: 0.1577\n",
      "Epoch:  13 | batch train loss: 0.1469\n",
      "Epoch:  13 | batch train loss: 0.1419\n",
      "Epoch:  13 -- total train loss: 0.1457=0.1236+0.0144+0.0077\n",
      "Epoch:  14 | batch train loss: 0.1511\n",
      "Epoch:  14 | batch train loss: 0.1496\n",
      "Epoch:  14 | batch train loss: 0.1479\n",
      "Epoch:  14 -- total train loss: 0.1435=0.1232+0.0132+0.0071\n",
      "Epoch:  15 | batch train loss: 0.1412\n",
      "Epoch:  15 | batch train loss: 0.1346\n",
      "Epoch:  15 | batch train loss: 0.1399\n",
      "Epoch:  15 -- total train loss: 0.1420=0.1232+0.0122+0.0067\n",
      "Epoch:  16 | batch train loss: 0.1568\n",
      "Epoch:  16 | batch train loss: 0.1346\n",
      "Epoch:  16 | batch train loss: 0.1418\n",
      "Epoch:  16 -- total train loss: 0.1411=0.1235+0.0113+0.0063\n",
      "Epoch:  17 | batch train loss: 0.1418\n",
      "Epoch:  17 | batch train loss: 0.1358\n",
      "Epoch:  17 | batch train loss: 0.1418\n",
      "Epoch:  17 -- total train loss: 0.1398=0.1232+0.0106+0.0060\n",
      "Epoch:  18 | batch train loss: 0.1438\n",
      "Epoch:  18 | batch train loss: 0.1309\n",
      "Epoch:  18 | batch train loss: 0.1418\n",
      "Epoch:  18 -- total train loss: 0.1385=0.1228+0.0099+0.0058\n",
      "Epoch:  19 | batch train loss: 0.1301\n",
      "Epoch:  19 | batch train loss: 0.1365\n",
      "Epoch:  19 | batch train loss: 0.1390\n",
      "Epoch:  19 -- total train loss: 0.1378=0.1228+0.0094+0.0056\n"
     ]
    }
   ],
   "source": [
    "train_autoencoder(autoencoder, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/alex.wolf/miniconda3/envs/sr_work/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type MaskedAutoencoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(autoencoder, 'auto_masked.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA1 = 0.22\n",
    "ALPHA2 = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLinAutoencoder(nn.Module):\n",
    "    def __init__(self, n_vars, n_terms, n_latent, estimator='ST', f_eval='Mode'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            MaskedLinear(n_vars, n_terms, estimator, f_eval, bias=False),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(n_terms, n_latent)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(n_latent, n_terms),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(n_terms, n_vars),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_func(I, alpha1, alpha2):\n",
    "    l2_loss = nn.MSELoss()\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def regularized_loss(X, Y, logits):\n",
    "        sigm1 = torch.sigmoid(logits)\n",
    "        return l2_loss(X, Y), alpha1*bce_loss(logits, I.t())+alpha2*torch.mean(sigm1*(1-sigm1))\n",
    "    \n",
    "    return regularized_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(autoencoder, loss_func):\n",
    "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=LR)\n",
    "    \n",
    "    t_X = torch.from_numpy(adata.X)\n",
    "    \n",
    "    logits = autoencoder.encoder[0].logits\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        autoencoder.train()\n",
    "        for step in range(int(adata.n_obs/BATCH_SIZE)):\n",
    "            batch = torch.from_numpy(adata.chunk_X(BATCH_SIZE))\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = 0\n",
    "            \n",
    "            for sample in batch:\n",
    "                sample = sample[None, :]\n",
    "                encoded, decoded = autoencoder(sample)\n",
    "                \n",
    "                loss = sum(loss_func(decoded, sample, logits))/BATCH_SIZE\n",
    "                loss.backward()\n",
    "                \n",
    "                batch_loss += loss.data\n",
    "            \n",
    "            optimizer.step()\n",
    "            if step % 100 == 0: print('Epoch: ', epoch, '| batch train loss: %.4f' % batch_loss.numpy())\n",
    "        autoencoder.eval()\n",
    "        _, t_decoded = autoencoder(t_X)\n",
    "        \n",
    "        t_loss = loss_func(t_decoded, t_X, logits)\n",
    "        t_loss = [sum(t_loss)] + list(t_loss)\n",
    "        t_loss = [l.data.numpy() for l in t_loss]\n",
    "        \n",
    "        print('Epoch: ', epoch, '-- total train loss: %.4f=%.4f+%.4f' % tuple(t_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = MaskedLinAutoencoder(adata.n_vars, len(adata.uns['terms']), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = get_loss_func(I, ALPHA1, ALPHA2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | batch train loss: 0.3905\n",
      "Epoch:  0 | batch train loss: 0.3084\n",
      "Epoch:  0 | batch train loss: 0.2768\n",
      "Epoch:  0 -- total train loss: 0.2674=0.1416+0.1258\n",
      "Epoch:  1 | batch train loss: 0.2629\n",
      "Epoch:  1 | batch train loss: 0.2459\n",
      "Epoch:  1 | batch train loss: 0.2324\n",
      "Epoch:  1 -- total train loss: 0.2229=0.1323+0.0906\n",
      "Epoch:  2 | batch train loss: 0.2225\n",
      "Epoch:  2 | batch train loss: 0.2020\n",
      "Epoch:  2 | batch train loss: 0.1963\n",
      "Epoch:  2 -- total train loss: 0.1962=0.1281+0.0681\n",
      "Epoch:  3 | batch train loss: 0.2025\n",
      "Epoch:  3 | batch train loss: 0.1896\n",
      "Epoch:  3 | batch train loss: 0.1774\n",
      "Epoch:  3 -- total train loss: 0.1794=0.1266+0.0529\n",
      "Epoch:  4 | batch train loss: 0.1751\n",
      "Epoch:  4 | batch train loss: 0.1801\n",
      "Epoch:  4 | batch train loss: 0.1578\n",
      "Epoch:  4 -- total train loss: 0.1674=0.1251+0.0423\n",
      "Epoch:  5 | batch train loss: 0.1564\n",
      "Epoch:  5 | batch train loss: 0.1700\n",
      "Epoch:  5 | batch train loss: 0.1507\n",
      "Epoch:  5 -- total train loss: 0.1592=0.1246+0.0346\n",
      "Epoch:  6 | batch train loss: 0.1547\n",
      "Epoch:  6 | batch train loss: 0.1551\n",
      "Epoch:  6 | batch train loss: 0.1547\n",
      "Epoch:  6 -- total train loss: 0.1541=0.1252+0.0289\n",
      "Epoch:  7 | batch train loss: 0.1647\n",
      "Epoch:  7 | batch train loss: 0.1521\n",
      "Epoch:  7 | batch train loss: 0.1515\n",
      "Epoch:  7 -- total train loss: 0.1482=0.1236+0.0246\n",
      "Epoch:  8 | batch train loss: 0.1457\n",
      "Epoch:  8 | batch train loss: 0.1453\n",
      "Epoch:  8 | batch train loss: 0.1483\n",
      "Epoch:  8 -- total train loss: 0.1446=0.1234+0.0212\n",
      "Epoch:  9 | batch train loss: 0.1469\n",
      "Epoch:  9 | batch train loss: 0.1471\n",
      "Epoch:  9 | batch train loss: 0.1485\n",
      "Epoch:  9 -- total train loss: 0.1418=0.1233+0.0185\n",
      "Epoch:  10 | batch train loss: 0.1422\n",
      "Epoch:  10 | batch train loss: 0.1471\n",
      "Epoch:  10 | batch train loss: 0.1458\n",
      "Epoch:  10 -- total train loss: 0.1398=0.1234+0.0164\n",
      "Epoch:  11 | batch train loss: 0.1350\n",
      "Epoch:  11 | batch train loss: 0.1298\n",
      "Epoch:  11 | batch train loss: 0.1490\n",
      "Epoch:  11 -- total train loss: 0.1386=0.1239+0.0147\n",
      "Epoch:  12 | batch train loss: 0.1268\n",
      "Epoch:  12 | batch train loss: 0.1452\n",
      "Epoch:  12 | batch train loss: 0.1373\n",
      "Epoch:  12 -- total train loss: 0.1363=0.1230+0.0133\n",
      "Epoch:  13 | batch train loss: 0.1303\n",
      "Epoch:  13 | batch train loss: 0.1478\n",
      "Epoch:  13 | batch train loss: 0.1313\n",
      "Epoch:  13 -- total train loss: 0.1347=0.1225+0.0122\n",
      "Epoch:  14 | batch train loss: 0.1261\n",
      "Epoch:  14 | batch train loss: 0.1258\n",
      "Epoch:  14 | batch train loss: 0.1415\n",
      "Epoch:  14 -- total train loss: 0.1338=0.1225+0.0112\n",
      "Epoch:  15 | batch train loss: 0.1380\n",
      "Epoch:  15 | batch train loss: 0.1236\n",
      "Epoch:  15 | batch train loss: 0.1340\n",
      "Epoch:  15 -- total train loss: 0.1332=0.1228+0.0104\n",
      "Epoch:  16 | batch train loss: 0.1297\n",
      "Epoch:  16 | batch train loss: 0.1343\n",
      "Epoch:  16 | batch train loss: 0.1345\n",
      "Epoch:  16 -- total train loss: 0.1321=0.1223+0.0097\n",
      "Epoch:  17 | batch train loss: 0.1285\n",
      "Epoch:  17 | batch train loss: 0.1296\n",
      "Epoch:  17 | batch train loss: 0.1365\n",
      "Epoch:  17 -- total train loss: 0.1317=0.1226+0.0091\n",
      "Epoch:  18 | batch train loss: 0.1322\n",
      "Epoch:  18 | batch train loss: 0.1237\n",
      "Epoch:  18 | batch train loss: 0.1311\n",
      "Epoch:  18 -- total train loss: 0.1311=0.1224+0.0086\n",
      "Epoch:  19 | batch train loss: 0.1382\n",
      "Epoch:  19 | batch train loss: 0.1365\n",
      "Epoch:  19 | batch train loss: 0.1333\n",
      "Epoch:  19 -- total train loss: 0.1304=0.1222+0.0082\n"
     ]
    }
   ],
   "source": [
    "train_autoencoder(autoencoder, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/alex.wolf/miniconda3/envs/sr_work/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type MaskedLinAutoencoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(autoencoder, 'auto_masked_2nd_lin.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetAutoencoder(nn.Module):\n",
    "    def __init__(self, I, n_vars, n_terms, n_latent):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            DetMaskLinear(I, n_vars, n_terms, bias=False),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(n_terms, n_latent)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(n_latent, n_terms),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(n_terms, n_vars),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(autoencoder, loss_func):\n",
    "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=LR)\n",
    "\n",
    "    t_X = torch.from_numpy(adata.X)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "\n",
    "        for step in range(int(adata.n_obs/BATCH_SIZE)):\n",
    "            X = torch.from_numpy(adata.chunk_X(BATCH_SIZE))\n",
    "            encoded, decoded = autoencoder(X)\n",
    "            loss = loss_func(decoded, X)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if step % 100 == 0: print('Epoch: ', epoch, '| batch train loss: %.4f' % loss.data.numpy())\n",
    "\n",
    "        _, t_decoded = autoencoder(t_X)\n",
    "        t_loss = loss_func(t_decoded, t_X)\n",
    "        print('Epoch: ', epoch, '-- total train loss: %.4f' % t_loss.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = DetAutoencoder(I, adata.n_vars, len(adata.uns['terms']), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | batch train loss: 0.1894\n",
      "Epoch:  0 | batch train loss: 0.1301\n",
      "Epoch:  0 | batch train loss: 0.1262\n",
      "Epoch:  0 -- total train loss: 0.1284\n",
      "Epoch:  1 | batch train loss: 0.1197\n",
      "Epoch:  1 | batch train loss: 0.1335\n",
      "Epoch:  1 | batch train loss: 0.1250\n",
      "Epoch:  1 -- total train loss: 0.1258\n",
      "Epoch:  2 | batch train loss: 0.1161\n",
      "Epoch:  2 | batch train loss: 0.1257\n",
      "Epoch:  2 | batch train loss: 0.1236\n",
      "Epoch:  2 -- total train loss: 0.1249\n",
      "Epoch:  3 | batch train loss: 0.1270\n",
      "Epoch:  3 | batch train loss: 0.1179\n",
      "Epoch:  3 | batch train loss: 0.1267\n",
      "Epoch:  3 -- total train loss: 0.1243\n",
      "Epoch:  4 | batch train loss: 0.1222\n",
      "Epoch:  4 | batch train loss: 0.1229\n",
      "Epoch:  4 | batch train loss: 0.1177\n",
      "Epoch:  4 -- total train loss: 0.1237\n",
      "Epoch:  5 | batch train loss: 0.1168\n",
      "Epoch:  5 | batch train loss: 0.1215\n",
      "Epoch:  5 | batch train loss: 0.1213\n",
      "Epoch:  5 -- total train loss: 0.1234\n",
      "Epoch:  6 | batch train loss: 0.1217\n",
      "Epoch:  6 | batch train loss: 0.1269\n",
      "Epoch:  6 | batch train loss: 0.1178\n",
      "Epoch:  6 -- total train loss: 0.1231\n",
      "Epoch:  7 | batch train loss: 0.1299\n",
      "Epoch:  7 | batch train loss: 0.1384\n",
      "Epoch:  7 | batch train loss: 0.1206\n",
      "Epoch:  7 -- total train loss: 0.1227\n",
      "Epoch:  8 | batch train loss: 0.1256\n",
      "Epoch:  8 | batch train loss: 0.1339\n",
      "Epoch:  8 | batch train loss: 0.1326\n",
      "Epoch:  8 -- total train loss: 0.1229\n",
      "Epoch:  9 | batch train loss: 0.1175\n",
      "Epoch:  9 | batch train loss: 0.1270\n",
      "Epoch:  9 | batch train loss: 0.1232\n",
      "Epoch:  9 -- total train loss: 0.1227\n",
      "Epoch:  10 | batch train loss: 0.1113\n",
      "Epoch:  10 | batch train loss: 0.1108\n",
      "Epoch:  10 | batch train loss: 0.1261\n",
      "Epoch:  10 -- total train loss: 0.1227\n",
      "Epoch:  11 | batch train loss: 0.1195\n",
      "Epoch:  11 | batch train loss: 0.1349\n",
      "Epoch:  11 | batch train loss: 0.1203\n",
      "Epoch:  11 -- total train loss: 0.1226\n",
      "Epoch:  12 | batch train loss: 0.1199\n",
      "Epoch:  12 | batch train loss: 0.1271\n",
      "Epoch:  12 | batch train loss: 0.1175\n",
      "Epoch:  12 -- total train loss: 0.1226\n",
      "Epoch:  13 | batch train loss: 0.1356\n",
      "Epoch:  13 | batch train loss: 0.1197\n",
      "Epoch:  13 | batch train loss: 0.1137\n",
      "Epoch:  13 -- total train loss: 0.1224\n",
      "Epoch:  14 | batch train loss: 0.1245\n",
      "Epoch:  14 | batch train loss: 0.1147\n",
      "Epoch:  14 | batch train loss: 0.1267\n",
      "Epoch:  14 -- total train loss: 0.1225\n",
      "Epoch:  15 | batch train loss: 0.1260\n",
      "Epoch:  15 | batch train loss: 0.1232\n",
      "Epoch:  15 | batch train loss: 0.1222\n",
      "Epoch:  15 -- total train loss: 0.1222\n",
      "Epoch:  16 | batch train loss: 0.1177\n",
      "Epoch:  16 | batch train loss: 0.1353\n",
      "Epoch:  16 | batch train loss: 0.1235\n",
      "Epoch:  16 -- total train loss: 0.1224\n",
      "Epoch:  17 | batch train loss: 0.1202\n",
      "Epoch:  17 | batch train loss: 0.1297\n",
      "Epoch:  17 | batch train loss: 0.1194\n",
      "Epoch:  17 -- total train loss: 0.1223\n",
      "Epoch:  18 | batch train loss: 0.1259\n",
      "Epoch:  18 | batch train loss: 0.1220\n",
      "Epoch:  18 | batch train loss: 0.1232\n",
      "Epoch:  18 -- total train loss: 0.1225\n",
      "Epoch:  19 | batch train loss: 0.1182\n",
      "Epoch:  19 | batch train loss: 0.1244\n",
      "Epoch:  19 | batch train loss: 0.1316\n",
      "Epoch:  19 -- total train loss: 0.1224\n"
     ]
    }
   ],
   "source": [
    "train_autoencoder(autoencoder, nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/alex.wolf/miniconda3/envs/sr_work/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DetAutoencoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(autoencoder, 'auto_masked_det.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
